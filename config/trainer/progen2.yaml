# logger
overwrite_output_dir: True
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
save_total_limit: 1
report_to: wandb

# training
num_train_epochs: 500
learning_rate: 2e-4
weight_decay: 0.1
max_grad_norm: 0.8
warmup_steps: 10000
fp16: False
optim: adamw_torch
lr_scheduler_type: linear

# ddp
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
dataloader_num_workers: 0
dataloader_pin_memory: True

# evaluation
per_device_eval_batch_size: 4
# evaluation_strategy: epoch
evaluation_strategy: steps
eval_steps: 10
