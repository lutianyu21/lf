# epoch:              20000
# base_lr:            1e-3
# warmup_steps:       200

# logger
output_dir: /AIRvePFS/ai4science/users/tianyu/LLMFolding-structure/output/checkpoints
overwrite_output_dir: True
logging_strategy: steps
logging_steps: 10
save_strategy: epoch
report_to: wandb

# training
num_train_epochs: 20
learning_rate: 1e-2
warmup_steps: 1000
fp16: False
optim: adamw_torch
lr_scheduler_type: linear

# ddp
per_device_train_batch_size: 4
gradient_accumulation_steps: 1
dataloader_num_workers: 0
dataloader_pin_memory: True

# evaluation
per_device_eval_batch_size: 4
# evaluation_strategy: epoch
evaluation_strategy: steps
eval_steps: 1000
