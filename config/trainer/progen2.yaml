# logger
overwrite_output_dir: True
logging_strategy: steps
logging_steps: 1
save_strategy: epoch
save_total_limit: 1
report_to: wandb
output_dir: dummy

# training
num_train_epochs: 1000
learning_rate: 2e-5
weight_decay: 0.01
# max_grad_norm: 1.0
warmup_steps: 1000
fp16: False
optim: adamw_torch
lr_scheduler_type: linear
# gradient_accumulation_steps: 8

# ddp
per_device_train_batch_size: 8
dataloader_num_workers: 0
dataloader_pin_memory: True

# evaluation
per_device_eval_batch_size: 8
evaluation_strategy: steps
eval_steps: 100
